<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>The Sixth Summer School on Statistical Methods for Linguistics and Psychology</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/lorikeet.jpg">

</head>
<body>

<section class="about">
  <div class="container">
    <h4>Welcome to the Sixth Summer School on Statistical Methods for Linguistics and Psychology, 12-16 September 2022</h4>

    <div class="row">
     </div>

    <br><br><br>
    <div>

        <img src="images/smlp2022photo.jpg", align="right",  height="540">

<br><br>
    <h3>Application, dates, location</h3>
        <ul>
          <li>Dates: 12-16 September 2022.</li>
          <li>Times: 9AM-5PM daily.</li>
          <li>Location: The summer school will be held at the <a hef="https://www.uni-potsdam.de/fileadmin/projects/zim/files/lageplaene_uni/Lageplan_Legende_GRS.PDF">Griebnitzsee campus in Potsdam</a>, at Haus 6. For train connections, consult <a href="https://www.bvg.de/en">bvg.de</a>; the train station near the campus is called Griebnitzsee Bhf.
</li>
          <li><strong>Application period: 17 Sept 2021 to 1 April 2022</strong>. Applications are closed. Decisions will be announced around 15 April 2022.

<!--
          <iframe width="560" height="315" src="https://www.youtube.com/embed/pAnGkj0z8V4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
-->

          </li>
          </ul>
</div>

<div><h3>Free online course: Introduction to Bayesian Data Analysis</h3>

You can do this four-week online course for free (starts Jan 25, 2022).
<a href="https://open.hpi.de/courses/bayesian-statistics2023">Details here</a>.

</div>

<div>
     <h3>Brief history of the summer school, and motivation</h3>

The summer school was started by Shravan Vasishth in 2017, as part of a <a href="https://www.uni-potsdam.de/en/sfb1287/projects/infrastructure-service-training-and-central-projects/project-q.html">methods project</a> funded within the <a href="https://www.uni-potsdam.de/sfb1287/index.html">SFB 1287</a>. The summer school aims to fill a gap in statistics education, specifically within the fields of linguistics and psychology. One goal of the summer school is to provide comprehensive training in the theory and application of statistics, with a special focus on the linear mixed model.  Another major goal is to make Bayesian data analysis a standard part of the toolkit for the linguistics and psychology researcher. Over time, the summer school has evolved to have at least four parallel streams: beginning and advanced courses in frequentist and Bayesian statistics. These may be expanded to more parallel sessions in future editions. We typically admit a total of 120 participants (in 2019, we had some 450 applications). In addition to the all-day courses, we regularly invite speakers to give lectures on important current issues relating to statistics. Previous editions of the summer school:
<a href="https://vasishth.github.io/smlp2021/">2021</a>,
<a href="https://vasishth.github.io/smlp2020/">2020</a>, <a href="https://vasishth.github.io/smlp2019/">2019</a>, <a href="https://vasishth.github.io/SMLP2018/">2018</a>, <a href="https://vasishth.github.io/SMLP2017/">2017</a>.
</div>


<div>
     <h3>Code of conduct</h3>

All participants will be expected to follow the (<a href="http://mc-stan.org/events/stancon2018/stancon-code_of_conduct.html">code of conduct</a>, taken from <a href="http://mc-stan.org/events/stancon2018/">StanCon 2018</a>. In case a participant has any concerns, please contact any of the following instructors: Audrey Bürki, Anna Laurinavichyute, Shravan Vasishth, Bruno Nicenboim, or Reinhold Kliegl.

</div>

<div>
     <h3>Invited lecturers</h3>

<a href="https://www.uni-potsdam.de/de/expbiopsy/staff/ralf-engbert">Ralf Engbert</a>; <a href="https://phillipalday.com/">Phillip Alday</a>;
<a href="https://stat.wisc.edu/staff/bates-douglas/">Douglas Bates</a> (Prof Bates is attending over zoom).


</div>


<div>
     <h3>Invited keynote speakers</h3>

<ol><li><a href="https://www.cl.uzh.ch/de/people/team/digital-linguistics/jaeger.html">Prof. Dr. Lena Jäger</a>, Zürich, Switzerland. (Tuesday, 13 Sept 2022, 5-6PM).<br>


Title: Bayesian Estimation of Measurement Reliability of Individual Differences in Sentence Processing
<br>
Abstract:
<br>
Theories of human sentence processing generally assume that the cognitive mechanisms involved in language processing are qualitatively identical across speakers.  However, over the past decade, evidence has accumulated indicating that individual differences in a comprehender’s cognitive capacities play an important role in sentence processing (e.g., Vuong and Martin, 2014; Nicenboim et al., 2015; Farmer et al., 2017). From a methodological point of view, the first step for a principled investigation of individual differences in sentence processing is to establish their test-retest measurement reliability, that is, the correlation of subject-level effects across multiple experimental sessions (Parsons et al., 2019; Cunnings and Fujita, 2020). We can’t take this measurement reliability as a given because of the so-called reliability paradox which states that  test-retest measurement reliability at the individual level is necessarily lower for manipulations with high between-subjects reliability, that is, replicability at the group level (Hedge, 2017). However, it is likely that precisely effects with high replicability at the group level constitute the set of well-established psycholinguistic phenomena which build the foundation of sentence processing theories.

In this talk, I will present ongoing work in which we assess the measurement reliability of individual differences in a range of theoretically relevant phenomena within and across methods.  We collected the first  cross-methodological reading corpus with multiple experimental sessions from each participant. 50 native speakers of German each participated in four experimental sessions (two eye-tracking and two self-paced reading sessions, 200 sessions in total). Participants read 80 pages of natural text (20 pages per session), and  participated in a comprehensive psychometric assessment measuring verbal and non-verbal working memory capacity, cognitive control and IQ, as well as lexical and non-lexical reading fluency. We estimate within- and cross-methodological test-retest measurement reliability of individual differences in well-established psycholinguistic effects by computing Bayesian correlation estimates (Matzke et al., 2017) of participant-specific random slopes between sessions from the same method, and sessions from different methods, respectively. We further explore whether cognitive capacities affect test-retest measurement reliability of individual-level effects.

We find that lexical-level effects are very stable within individuals across sessions and methods (e.g., participants exhibiting a particularly strong word length effect do so across sessions and methods). By contrast, higher-level cognitive effects that involve syntactic processing (e.g., surprisal or dependency locality) are less stable within individuals. In a nutshell, we find that for higher-level effects, test-retest measurement reliability of individual-level effects is generally higher in self-paced reading than in eye-tracking. Cross-methodological measurement reliability of individual-level effects is generally low for all eye-tracking measures. Future work will need to address the question whether the observed low test-retest measurement reliability of higher-level cognitive effects can be explained by the stimulus materials (naturalistic texts) which, in contrast to minimal pairs of planned experiments, do not push the comprehender’s sentence processor to its limits, and therefore might be less adequate to assess individual differences in sentence processing.





</li>

<li><a href="https://pure.au.dk/portal/en/persons/riccardo-fusaroli(3f72f2a1-e93a-4689-872c-c11c9703c1cc).html">Prof. Dr. Riccardo Fusaroli</a>. (Thursday, 15 Sept 2022, 5-6PM).
<br>
Title: Standing on the shoulders of normal-sized people. Promise and challenges of cumulative statistical approaches
<br>
Abstract:
<br>

We often hear that Newton stood on the shoulders of giants and that science is a cumulative enterprise where new research builds on previous results. This conception of science relates to a commonly cited benefit of Bayesian approaches: their ability to integrate diverse sources of information, e.g. results of previous studies as informed priors. However, this practice is rarely seen in the literature. One possible explanation could be that we remain skeptic of scientific findings in our own field; that is, we know that we stand on the shoulders of normal-sized, fallible people (just as we ourselves are), rather than on the shoulders of giants. This raises the question of how we best integrate fallible findings from previous analyses in our studies.



In this talk I will tackle this issue using a combination of simplified simulations, and the concerns arisen in concrete studies using informed priors.



First I will cover simulation-based studies of posterior passing: what happens when we use previous posterior estimates as priors, in a sterilized in silico environment? I will then let the complexity of real research slowly creep in: from linear chains of one study following the other, to interrupted chains due to publication bias, to meandering forking paths where studies know and include only some of the literature. These simulations show that posterior passing is slowed down by complexity, but still provides the best solution for this cumulative enterprise.



With the simulations at hand, I will turn to real application scenarios, where previous literature and expert opinions are used to build  informed priors. Novel concerns arise: hierarchical structures of expectations, heterogeneity of studies, undue levels of confidence, etc.



Based on these results, I will advocate for a critical use of informed priors, involving comparisons between informed priors and alternative (e.g. skeptical) priors, and explicit testing of inferential robustness.
</li>
</ol>

</div>

<div>
<h3>Curriculum and schedule</h3>

<strong>Schedule</strong>: Here is the schedule as a <a href="./Schedule-SMLP2022.pdf">pdf</a>.
<br><br>

<strong>Social hours</strong>: All participants are invited to an evening of
snacks and general hanging out together on Monday and Wednesday 5PM onwards (CEST) .
<br>

We offer foundational/introductory and advanced courses in Bayesian and frequentist statistics. When applying, participants are expected to choose only one stream.
This year, there will be a special series of lectures by Ralf Engbert that everyone is welcome to attend.
<br>
<br>

<ul>
    <li><strong>Special short course: Introduction to Dynamical Models in Cognitive Science</strong> (all participants are welcome, no need to register). Taught by Ralf Engbert, assisted by Lisa Schwetlick and Maximilian Rabe. Tuesday and Thursday, 3:00-4:30PM. Location: Hoersaal 03.<br><br>

    This course is an introduction to dynamical modeling of eye movements
    during reading. Lecture I (Tuesday) starts with an introduction to the basic concepts of mathematical modeling using ordinary differential equations. We develop a simplified version of the SWIFT model for eye guidance in reading (Engbert et al., 2005; Rabe et al., 2021), including the computer implementation with R.
    Lecture II (Thursday) introduces sequential likelihood methods for dynamical processes. We show that the likelihood function can
    be decomposed into temporal and spatial components. For the simplified
    SWIFT model, we carry out numerical computations of the likelihood
    function.<br> <br>

    Course material: All slides and computer code will be made available via
    OSF at <a href="https://osf.io/8wrf6/">https://osf.io/8wrf6/</a>
    <br><br>


Timing: Tuesday and Thursday: 3:00-4:30PM.<br>

</li>

<li><strong>Introduction to Bayesian data analysis</strong> (maximum 30 participants). Taught by <a href="https://vasishth.github.io/">Shravan Vasishth</a>, assisted by <a href="https://annlaurin.github.io/">Anna Laurinavichyute</a>.</li>

This course is an introduction to Bayesian modeling, oriented towards linguists and psychologists.  Topics to be covered: Introduction to Bayesian data analysis, Linear Modeling, Hierarchical Models. We will cover these topics within the context of an applied Bayesian workflow that includes exploratory data analysis, model fitting, and model checking using simulation. Participants are expected to be familiar with R, and must have some experience in data analysis, particularly with the R library lme4.<br>

<strong>Course Materials</strong>

Previous year's course web page: <a href="https://vasishth.github.io/IntroBayesSMLP2021">all materials (videos etc.) from the previous year are available here</a>.<br>

Textbook: <a href="https://vasishth.github.io/bayescogsci/">here</a>. We will work through the first six chapters.<br>

<br><br>
<li><strong>Advanced Bayesian data analysis</strong> (maximum 30 participants). Taught by <a href="https://bnicenboim.github.io/">Bruno Nicenboim</a>, assisted by <a href="https://sites.google.com/site/himanshuyadavjnu/">Himanshu Yadav</a></li>

This course assumes that participants have some experience in Bayesian modeling already using brms and want to transition to Stan to learn more advanced methods and start building simple computational cognitive models. Participants should have worked through or be familiar with the material in the first five chapters of our book draft: <a href="https://vasishth.github.io/Bayes_CogSci/">Introduction to Bayesian Data Analysis for Cognitive Science</a>. In this course, we will cover Parts III to V of our book draft: model comparison using Bayes factors and k-fold cross validation, introduction and relatively advanced models with Stan, and simple computational cognitive models.
<br>

<strong>Course Materials</strong>

Textbook <a href="https://vasishth.github.io/bayescogsci/">here</a>. We will start from Part III of the book (Advanced models with Stan). Participants are expected to be familiar with the first five chapters.<br><br>

<li><strong>Foundational methods in frequentist statistics</strong> (maximum 30 participants). Taught by  <a href="https://www.uni-potsdam.de/en/ling/staff-list/audreybuerki.html">Audrey Buerki</a>, <a href="https://www.health-and-medical-university.de/universitaet/unser-team/team-fakultaet-gesundheit/professoren/prof-dr-daniel-schad-hmu-potsdam/">Daniel Schad</a>, and <a href="https://www.jverissimo.net/">João Veríssimo</a>.</li>

Participants will be expected to have used linear mixed models before, to the level of the textbook by <a href="http://www.bodowinter.com/blog/book-release-statistics-for-linguists">Winter (2019, Statistics for Linguists)</a>, and want to acquire a deeper knowledge of frequentist foundations, and understand the linear mixed modeling framework more deeply. Participants are also expected to have fit multiple regressions. We will cover model selection, contrast coding, with a heavy emphasis on simulations to compute power and to understand what the model implies. We will work on (at least some of) the participants' own datasets. <i>This course is not appropriate for researchers new to R or to frequentist statistics</i>.<br>

<strong>Course Materials</strong>

Textbook draft <a href="https://vasishth.github.io/Freq_CogSci/">here</a>.
<br><br>

<li><strong>Advanced methods in frequentist statistics with Julia</strong> (maximum 30 participants). Taught by  <a href="https://www.uni-potsdam.de/en/trainingswissenschaft/staff/rkliegl.html">Reinhold Kliegl</a>, <a href="https://phillipalday.com/">Phillip Alday</a>, and (over zoom:) <a href="https://stat.wisc.edu/staff/bates-douglas/">Doug Bates</a>. </li>

</ul>

Applicants must have experience with linear mixed models and be interested in learning how to carry out such analyses with the <a href="https://github.com/JuliaStats/MixedModels.jl">Julia-based MixedModels.jl package</a>) (i.e., the analogue of the R-based lme4 package). MixedModels.jl has some significant advantages. Some of them are: (a) new and more efficient computational implementation, (b) speed — needed for, e.g.,  complex designs and power simulations,
(c) more flexibility for selection of parsimonious mixed models, and
(d) more flexibility in taking into account autocorrelations or other dependencies — typical  EEG-, fMRI-based time series (under development).

We <strong>do not expect</strong> profound knowledge of Julia from participants; the necessary subset of knowledge will be taught on the first day of the course. We do expect a readiness to <a href="https://julialang.org/downloads/">install Julia</a> and the confidence that with some basic instruction participants will be able to adapt prepared Julia scripts for their own data or to adapt some of their own lme4-commands to the equivalent MixedModels.jl-commands. The course will be taught in a hybrid IDE. There is already the option to execute R chunks from within Julia, meaning one needs Julia primarily for execution of MixedModels.jl commands as replacement of lme4. There is also an option to call MixedModels.jl from within R and process the resulting object like an lme4-object. Thus, much of pre- and postprocessing (e.g., data simulation for complex experimental designs; visualization of partial-effect interactions or shrinkage effects) can be carried out in R.
<br>

<strong>Course Materials</strong>

Github repo: <a href="https://github.com/RePsychLing/SMLP2022">here</a>.<br><br>

</div>


<div>
<h3>Fees and accommodation</h3>

If the summer school is held in person (as is the plan), there will be a 40 Euro fee; this covers costs for coffee and snacks. Participants who are accepted are expected to arrange their own accommodation. We strongly advise participants to find a place to stay near Griebnitzsee campus, and not in Berlin. The reason is that German train personnel tend to go on strike every year around the time of the summer school. You will be better off if you can get easily to the Griebnitzsee campus.
<br><br>
</div>


<div>
<h3>Contact details</h3>

For any questions regarding this summer school that have not been addressed on this home page already, please contact <a href="https://vasishth.github.io/">Shravan Vasishth</a>.
<br><br>
</div>

<div>
<h3>Funding</h3>

This summer school is funded by the DFG and is part of the <a href="https://www.uni-potsdam.de/sfb1287/index.html">SFB 1287, “Variability in Language and Its Limits”</a>.
</div>

</div>
<hr/>


</section>

</body>
</html>
